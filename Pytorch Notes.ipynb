{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f764ce",
   "metadata": {},
   "source": [
    "## Introductory Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2aa1be70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom tensor: tensor([[2.5000, 0.1000],\n",
      "        [3.0000, 4.0000]])\n",
      "Empty tensor: tensor([[[2.3694e-38, 2.3694e-38],\n",
      "         [2.3694e-38, 2.3694e-38]],\n",
      "\n",
      "        [[2.3694e-38, 2.3694e-38],\n",
      "         [7.6892e-06, 2.6950e-09]]])\n",
      "Ones tensor: tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "Rand tensor: tensor([[[[0.9578, 0.1140, 0.4125],\n",
      "          [0.1566, 0.8307, 0.8066]],\n",
      "\n",
      "         [[0.4926, 0.2406, 0.4073],\n",
      "          [0.6748, 0.7555, 0.8665]]],\n",
      "\n",
      "\n",
      "        [[[0.5896, 0.2643, 0.1685],\n",
      "          [0.8992, 0.3259, 0.6409]],\n",
      "\n",
      "         [[0.9130, 0.4511, 0.9156],\n",
      "          [0.3728, 0.3427, 0.1517]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Tensor initialization\n",
    "o = torch.tensor([[2.5, 0.1], [3, 4]])\n",
    "print(\"Custom tensor:\", o)\n",
    "\n",
    "x = torch.empty(2,2,2)\n",
    "print(\"Empty tensor:\", x)\n",
    "\n",
    "y = torch.ones(2,2)\n",
    "print(\"Ones tensor:\", y)\n",
    "\n",
    "z = torch.rand(2,2,2,3)\n",
    "print(\"Rand tensor:\", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e114397",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float64\n",
      "torch.Size([3, 3, 3])\n",
      "torch.Size([3, 3, 3])\n",
      "Res1: tensor([[[2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.]],\n",
      "\n",
      "        [[2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.]],\n",
      "\n",
      "        [[2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.]]], dtype=torch.float64)\n",
      "Res2: tensor([[[2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.]],\n",
      "\n",
      "        [[2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.]],\n",
      "\n",
      "        [[2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.]]], dtype=torch.float64)\n",
      "a: tensor([[[2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.]],\n",
      "\n",
      "        [[2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.]],\n",
      "\n",
      "        [[2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.]]])\n",
      "b: tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]], dtype=torch.float64)\n",
      "Slice1: tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "Slice2: tensor([2., 2., 2.])\n",
      "Get1: 2.0\n",
      "new_shape1: tensor([[[[2.],\n",
      "          [2.],\n",
      "          [2.]],\n",
      "\n",
      "         [[2.],\n",
      "          [2.],\n",
      "          [2.]],\n",
      "\n",
      "         [[2.],\n",
      "          [2.],\n",
      "          [2.]]],\n",
      "\n",
      "\n",
      "        [[[2.],\n",
      "          [2.],\n",
      "          [2.]],\n",
      "\n",
      "         [[2.],\n",
      "          [2.],\n",
      "          [2.]],\n",
      "\n",
      "         [[2.],\n",
      "          [2.],\n",
      "          [2.]]],\n",
      "\n",
      "\n",
      "        [[[2.],\n",
      "          [2.],\n",
      "          [2.]],\n",
      "\n",
      "         [[2.],\n",
      "          [2.],\n",
      "          [2.]],\n",
      "\n",
      "         [[2.],\n",
      "          [2.],\n",
      "          [2.]]]])\n",
      "new_shape2: tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "Numpy: [[2 5]\n",
      " [4 6]]\n",
      "Tensor: tensor([[2, 5],\n",
      "        [4, 6]], dtype=torch.int32)\n",
      "Tensor: tensor([[3, 0],\n",
      "        [9, 7]])\n",
      "Numpy: [[3 0]\n",
      " [9 7]]\n",
      "Tensor: tensor([[ 6,  0],\n",
      "        [18, 14]])\n",
      "Numpy: [[ 6  0]\n",
      " [18 14]]\n"
     ]
    }
   ],
   "source": [
    "# Tensor Methods and Operations\n",
    "\n",
    "# Tensor Dtypes\n",
    "# Available dtypes: torch.float(float32), torch.double(float64), torch.half(float16)\n",
    "# torch.uint8, torch.int8, torch.short(int16), torch.int(int32), torch.bool\n",
    "\n",
    "a = torch.ones(3,3,3)\n",
    "print(a.dtype)\n",
    "\n",
    "b = torch.ones(3,3,3, dtype=torch.double)\n",
    "print(b.dtype)\n",
    "\n",
    "# Tensor Size\n",
    "\n",
    "print(a.size())\n",
    "print(b.size())\n",
    "\n",
    "# Tensor elementwise operations\n",
    "# Basic operations are available, done in same way: \n",
    "\n",
    "res1 = a + b # Copy\n",
    "res2 = torch.add(a, b) # Copy\n",
    "a.add_(b) # Inplace a\n",
    "\n",
    "print(\"Res1:\", res1)\n",
    "print(\"Res2:\", res2)\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "\n",
    "# Slicing and accessing operations\n",
    "\n",
    "slice1 = a[:, 1]\n",
    "print(\"Slice1:\", slice1)\n",
    "slice2 = a[1, 1]\n",
    "print(\"Slice2:\", slice2)\n",
    "\n",
    "get1 = a[1, 1, 1].item() #works only if there is one item\n",
    "print(\"Get1:\", get1)\n",
    "\n",
    "# Morphing tensors\n",
    "new_shape1 = a.view(3,3,3,1) # Tensors should be compatible to original tensor\n",
    "new_shape2 = a.view(27) # Tensors should be compatible to original tensor\n",
    "print(\"new_shape1:\", new_shape1)\n",
    "print(\"new_shape2:\", new_shape2)\n",
    "\n",
    "# Conversion from and to numpy\n",
    "\n",
    "numpy_arr =  np.array([[2, 5], [4, 6]])\n",
    "print(\"Numpy:\", numpy_arr)\n",
    "numpy_to_tensor = torch.from_numpy(numpy_arr)\n",
    "print(\"Tensor:\", numpy_to_tensor)\n",
    "\n",
    "tensor_arr = torch.tensor([[3, 0], [9, 7]])\n",
    "print(\"Tensor:\", tensor_arr)\n",
    "tensor_to_numpy = tensor_arr.numpy()\n",
    "print(\"Numpy:\", tensor_to_numpy)\n",
    "\n",
    "# These arrays are of same memory location, modifying one will result in modification of other\n",
    "\n",
    "tensor_arr.add_(tensor_arr)\n",
    "print(\"Tensor:\", tensor_arr)\n",
    "print(\"Numpy:\", tensor_to_numpy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31018e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([ 0.0795, -1.5116,  0.5665], requires_grad=True)\n",
      "Forward pass: tensor([5.0795, 3.4884, 5.5665], grad_fn=<AddBackward0>)\n",
      "Scalar gradient: tensor([3.3863, 2.3256, 3.7110])\n",
      "Matrix gradient: tensor([33.8634, 37.2098, 70.5088])\n"
     ]
    }
   ],
   "source": [
    "# Autograd \n",
    "\n",
    "auto_tensor = torch.randn(3, requires_grad=True)\n",
    "print(\"Input:\", auto_tensor)\n",
    "\n",
    "add_y = auto_tensor + 5\n",
    "print(\"Forward pass:\", add_y)\n",
    "\n",
    "# When result is scalar\n",
    "mult_z = add_y * add_y\n",
    "mult_z = mult_z.mean()\n",
    "mult_z.backward() # dz/dx\n",
    "\n",
    "print(\"Scalar times gradient:\", auto_tensor.grad) \n",
    "\n",
    "# When result isn't scalar\n",
    "mult_w = add_y * add_y\n",
    "mult_w.backward(torch.tensor([3, 5, 6])) # dw/dx\n",
    "\n",
    "print(\"Matrix times gradient:\", auto_tensor.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a251ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-4.)\n"
     ]
    }
   ],
   "source": [
    "# Backpropagation\n",
    "\n",
    "x1 = torch.tensor(1.0)\n",
    "y1 = torch.tensor(3.0)\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# 1. Forward Pass\n",
    "y_hat = w * x1\n",
    "# 2. Compute Loss\n",
    "loss = (y_hat - y1) ** 2\n",
    "# 3. Backward Pass\n",
    "loss.backward()\n",
    "print(w.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0cec5678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506905\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "Prediction after training: f(5) = 9.997\n"
     ]
    }
   ],
   "source": [
    "# Manual gradient descent\n",
    "\n",
    "# Compute every step manually with numpy\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x \n",
    "\n",
    "# here : f = 2 * x\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model output\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "# J = MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N * 2x(w*x - y)\n",
    "def gradient(x, y, y_pred):\n",
    "    return np.mean(2*x*(y_pred - y))\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 50\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # calculate gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "\n",
    "    # update weights\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa7ad701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# Automate the gradient with torch\n",
    "\n",
    "# Here we replace the manually computed gradient with autograd\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x \n",
    "\n",
    "# here : f = 2 * x\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model output\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    #w.data = w.data - learning_rate * w.grad\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    \n",
    "    # zero the gradients after updating\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a096b841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch  1 : w =  tensor(0.3000, requires_grad=True)  loss =  tensor(30., grad_fn=<MseLossBackward0>)\n",
      "epoch  11 : w =  tensor(1.6653, requires_grad=True)  loss =  tensor(1.1628, grad_fn=<MseLossBackward0>)\n",
      "epoch  21 : w =  tensor(1.9341, requires_grad=True)  loss =  tensor(0.0451, grad_fn=<MseLossBackward0>)\n",
      "epoch  31 : w =  tensor(1.9870, requires_grad=True)  loss =  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "epoch  41 : w =  tensor(1.9974, requires_grad=True)  loss =  tensor(6.7705e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch  51 : w =  tensor(1.9995, requires_grad=True)  loss =  tensor(2.6244e-06, grad_fn=<MseLossBackward0>)\n",
      "epoch  61 : w =  tensor(1.9999, requires_grad=True)  loss =  tensor(1.0176e-07, grad_fn=<MseLossBackward0>)\n",
      "epoch  71 : w =  tensor(2.0000, requires_grad=True)  loss =  tensor(3.9742e-09, grad_fn=<MseLossBackward0>)\n",
      "epoch  81 : w =  tensor(2.0000, requires_grad=True)  loss =  tensor(1.4670e-10, grad_fn=<MseLossBackward0>)\n",
      "epoch  91 : w =  tensor(2.0000, requires_grad=True)  loss =  tensor(5.0768e-12, grad_fn=<MseLossBackward0>)\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# 1) Design model (input, output, forward pass with different layers)\n",
    "# 2) Construct loss and optimizer\n",
    "# 3) Training loop\n",
    "#       - Forward = compute prediction and loss\n",
    "#       - Backward = compute gradients\n",
    "#       - Update weights\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x \n",
    "\n",
    "# here : f = 2 * x\n",
    "\n",
    "# 0) Training samples\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "# 1) Design Model: Weights to optimize and forward function\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n",
    "\n",
    "# 2) Define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "# callable function\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_predicted = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('epoch ', epoch+1, ': w = ', w, ' loss = ', l)\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6df4ea73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 4, #features: 1\n",
      "Prediction before training: f(5) = 5.094\n",
      "epoch  1 : w =  1.0201683044433594  loss =  tensor(6.0524, grad_fn=<MseLossBackward0>)\n",
      "epoch  11 : w =  1.5861130952835083  loss =  tensor(0.2899, grad_fn=<MseLossBackward0>)\n",
      "epoch  21 : w =  1.6847151517868042  loss =  tensor(0.1330, grad_fn=<MseLossBackward0>)\n",
      "epoch  31 : w =  1.7079217433929443  loss =  tensor(0.1217, grad_fn=<MseLossBackward0>)\n",
      "epoch  41 : w =  1.718784213066101  loss =  tensor(0.1145, grad_fn=<MseLossBackward0>)\n",
      "epoch  51 : w =  1.7274504899978638  loss =  tensor(0.1078, grad_fn=<MseLossBackward0>)\n",
      "epoch  61 : w =  1.735559105873108  loss =  tensor(0.1016, grad_fn=<MseLossBackward0>)\n",
      "epoch  71 : w =  1.7433797121047974  loss =  tensor(0.0956, grad_fn=<MseLossBackward0>)\n",
      "epoch  81 : w =  1.7509615421295166  loss =  tensor(0.0901, grad_fn=<MseLossBackward0>)\n",
      "epoch  91 : w =  1.7583180665969849  loss =  tensor(0.0848, grad_fn=<MseLossBackward0>)\n",
      "Prediction after training: f(5) = 9.515\n"
     ]
    }
   ],
   "source": [
    "# 1) Design model (input, output, forward pass with different layers)\n",
    "# 2) Construct loss and optimizer\n",
    "# 3) Training loop\n",
    "#       - Forward = compute prediction and loss\n",
    "#       - Backward = compute gradients\n",
    "#       - Update weights\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x \n",
    "\n",
    "# here : f = 2 * x\n",
    "\n",
    "# 0) Training samples, watch the shape!\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(f'#samples: {n_samples}, #features: {n_features}')\n",
    "# 0) create a test sample\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "# 1) Design Model, the model has to implement the forward pass!\n",
    "# Here we can use a built-in model from PyTorch\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# we can call this model with samples X\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "'''\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define diferent layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "model = LinearRegression(input_size, output_size)\n",
    "'''\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# 2) Define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass with our model\n",
    "    y_predicted = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters() # unpack parameters\n",
    "        print('epoch ', epoch+1, ': w = ', w[0][0].item(), ' loss = ', l)\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
